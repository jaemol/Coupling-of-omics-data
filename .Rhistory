}
}
e
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
# loading in the sources of other functions
source("G:/Mit drev/Specialeprojekt/Programming/extracting_data_KAT.R")
source("G:/Mit drev/Specialeprojekt/Programming/data_filtering.R")
source("G:/Mit drev/Specialeprojekt/Programming/Functions.R")
# loading in the data
list[data, len_16s, len_qpcr] <- extracting_data_KAT()
?grep
infile = data
?var
# loading libraries
library(factoextra) # used to create ggplot2-based visualization
setwd("~/GitHub/Coupling-of-omics-data")
data_16s  <- read.table("Testdata/allData_16S_cleaned.txt")
data_qpcr <- read.table("Testdata/allData_qPCR_cleaned.txt")
# next, include only testID (PIG_DATE), OUA and the results of the analyses
pure_data_16s   <- subset(data_16s, select = -c(PIG, DATE, WEANING_TIME, GROUP, Newlytreated,
WEEK, SAMPLEWEEK, Florkem, Metacam, Zactran,
Antibiotic, Treatment_group, Treatment_date, CorrectedGroup,
PIG.1, DATE.1, OUA.1, PIG_DATE.1, OriginalNAME))
pure_data_qpcr  <- subset(data_qpcr, select = -c(PIG, DATE, WEANING_TIME, GROUP, Newlytreated,
WEEK, SAMPLEWEEK, Florkem, Metacam, Zactran,
Antibiotic, Treatment_group, Treatment_date, CorrectedGroup, sample))
# extract two arrays of testIDs, and find which are common between them both
testID_16s  <- pure_data_16s$PIG_DATE
testID_qpcr <- pure_data_qpcr$PIG_DATE
common_IDs  <- intersect(testID_16s, testID_qpcr)
# only keep relevant testIDs
pure_data_16s   = pure_data_16s[data_16s$PIG_DATE %in% common_IDs, ]
pure_data_qpcr  = pure_data_qpcr[data_qpcr$PIG_DATE %in% common_IDs, ]
# sort the datasets based testID, append whilst removing 1 edition of testID and OUA
pure_data_16s   = pure_data_16s[order(pure_data_16s$PIG_DATE), ]
pure_data_qpcr  = pure_data_qpcr[order(pure_data_qpcr$PIG_DATE), ]
pure_data_qpcr  = subset(pure_data_qpcr, select = -c(PIG_DATE, OUA))
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(data_16s)
newNames <- apply(stringr::str_split_fixed(string = origNames, pattern = "_",8)[,1:6],1, paste, collapse="_")
head(data_16s, 2)
head(data_16s[1:3], 2)
head(pure_data_16s[1:3], 2)
# extracting testID and OUA from 16s data
testID  <- pure_data_16s$PIG_DATE
OUA     <- pure_data_16s$OUA
pure_data_16s = subset(pure_data_16s, select = -c(PIG_DATE, OUA))
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(data_16s)
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(pure_data_16s)
newNames <- apply(stringr::str_split_fixed(string = origNames, pattern = "_",8)[,1:6],1, paste, collapse="_")
length(unique(newNames))
uniqNames=unique(newNames)
newDat=data.frame(dummy=1:NROW(pure_data_16s))
#j=uniqNames[1]
for(j in uniqNames) {
jIndx=grep(j,origNames )
if(length(jIndx)>1) {
newDat=cbind(newDat,rowSums(pure_data_16s[,jIndx]))
} else {
newDat=cbind(newDat,(pure_data_16s[,jIndx]))
}
}
newDat=newDat[,-1]
colnames(newDat)=uniqNames
head(newDat, 10)
View(newDat)
# now making that new data into 16s data set
pure_data_16s = newDat
# now, append the datasets, to get one set, along with testID and OUA
complete_data <- cbind(testID, OUA, pure_data_16s, pure_data_qpcr, deparse.level = 1)
View(complete_data)
# keeping the length of 16s and qpcr data - JUST the results, no ID or OUA
len_16s   <- length(data_16s[1,]) - 2
len_qpcr  <- length(data_qpcr[1,])
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
## Stationary
source("Programming/extracting_data_KAT.R")
source("Programming/data_filtering.R")
# loading in the data
list[data, len_16s, len_qpcr] <- extracting_data_KAT()
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(PIG_DATE, OUA))
inData = subset(inData, select = -c(testID, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
#mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
mmModel <- nls(sort(ratioData) ~ 1/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
#mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
mmModel <- nls(sort(ratioData) ~ Vm/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
#abline(h = threshold_ratio)
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
rm(list=setdiff(ls(), "outData"))
print("Filtering done...")
data <- outData
# loading libraries
library(fpc)
library(dbscan)
library(factoextra)
# setting seed
set.seed(123)
# excluding testID and OUA (antibiotics used or not)
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
}
}
inDataScale=scale(inData, center = T, scale = T)
findEpsi(inDataScale, minRange = 0, maxRange = 100, steps = 10,maxY=30)
dbscan::dbscan(scale(inData),10000,10)
findEpsi(inDataScale, minRange = 0, maxRange = 3, steps = 0.1,maxY=200)
findEpsi(inDataScale, minRange = 0, maxRange = 120, steps = 0.1,maxY=200)
dbscan::dbscan(scale(inData),10000,10)
findEpsi(inDataScale, minRange = 0, maxRange = 25, steps = 0.1 ,maxY=200)
?dbscan
findEpsi
dbscan::dbscan(scale(inData),10,10)
dbscan::dbscan(scale(inData),2,10)
dbscan::dbscan(scale(inData),20,10)
dbscan::dbscan(scale(inData),25,10)
dbscan::dbscan(scale(inData),50,10)
dbscan::dbscan(inDataScale,50,10)
dbscan::dbscan(inDataScale,11,10)
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 7, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 2, nstart = 25)
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 4, nstart = 25)
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 2, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 1, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 3, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 3, nstart = 50)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 3, nstart = 10)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# loading libraries
library(dbscan)
library(fpc)
library(factoextra)
# excluding testID and OUA (antibiotics used or not)
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
}
}
findEpsi(inDataScale, minRange = 0, maxRange = 25, steps = 0.1 ,maxY=200)
?dbscan::dbscan
dbscan::dbscan(inDataScale,11,10)
findEpsi(inDataScale, minRange = 0, maxRange = 35, steps = 0.1 ,maxY=200)
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
print(length(which(DB$cluster == 0)))
}
}
inDataScale=scale(inData, center = T, scale = T)
findEpsi(inDataScale, minRange = 0, maxRange = 35, steps = 0.1 ,maxY=200)
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
print(i,length(which(DB$cluster == 0)))
}
}
inDataScale=scale(inData, center = T, scale = T)
findEpsi(inDataScale, minRange = 0, maxRange = 35, steps = 1 ,maxY=200)
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
print(i)
print(length(which(DB$cluster == 0)))
}
}
inDataScale=scale(inData, center = T, scale = T)
findEpsi(inDataScale, minRange = 0, maxRange = 35, steps = 1 ,maxY=200)
findEpsi(inDataScale, minRange = 0, maxRange = 35, steps = 1 ,maxY=900)
findEpsi(inDataScale, minRange = 0, maxRange = 50, steps = 1 ,maxY=900)
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
print(i)
print(length(unique(DB$cluster)))
}
}
inDataScale=scale(inData, center = T, scale = T)
findEpsi(inDataScale, minRange = 0, maxRange = 50, steps = 1 ,maxY=900)
findEpsi(inDataScale, minRange = 0, maxRange = 65, steps = 1 ,maxY=900)
dbscan::dbscan(inDataScale,6,65)
# loading libraries
library(pbkrtest)
library(ggplot2)
library(ggpubr)
library(factoextra) # used to create ggplot2-based visualization
library(data.table) # used to keep structure of data frame when transposing
# excluding testID and OUA (antibiotics used or not)
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
# transposing data; samples on columns, features on rows
inData = transpose(inData)
# adding the names to the transposed data
rownames(inData) <- colnames(data[-(1:2)])
# making PCA model
pcaModel <- prcomp(inData, scale = TRUE)
# making scree plot, to visualize eigenvalues
fviz_eig(pcaModel)
# making a graph of the variables
# visualizing the correlation between the variables
# positive correlated variables point to the same side of the plot
# negative correlated variables point to opposite sides of the plot
fviz_pca_var(pcaModel,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = FALSE     # Avoid text overlapping
#,select.var = list(name = rownames(inData))
)
# excluding testID and OUA (antibiotics used or not)
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
# making PCA model
pcaModel <- prcomp(inData, scale = TRUE)
# making scree plot, to visualize eigenvalues
fviz_eig(pcaModel)
# making a graph of the variables
# visualizing the correlation between the variables
# positive correlated variables point to the same side of the plot
# negative correlated variables point to opposite sides of the plot
fviz_pca_var(pcaModel,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = FALSE     # Avoid text overlapping
#,select.var = list(name = rownames(inData))
)
