library(gsubfn)
options(warn = 0) # set=2 to end loops when warnings occur
data = complete_data
phaobacBin <- data$array.phaebac.bin
inData = subset(data, select = -c(array.phaebac.bin))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame),
start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
rm(list=setdiff(ls(), "outData"))
# loading NetCoMi library
library(NetCoMi)
data = outData
# building single network with SPRING as association measure - Full dataset, both treated and untreated
net_single_fullSet <- netConstruct(data,
#filtTax = "highestFreq",
#filtTaxPar = list(highestFreq = 100),
#filtSamp = "totalReads",
#filtSampPar = list(totalReads = 1000),
measure = "spearman",thresh = 0.6,
measurePar = list(nlambda=10,
rep.num=10),
normMethod = "none",
zeroMethod = "none",
sparsMethod = "threshold",
dissFunc = "signed",
verbose = 3,
seed = 123456)
props_single_fullSet <- netAnalyze(net_single_fullSet,
centrLCC = TRUE,
clustMethod = "cluster_fast_greedy",
hubPar = "eigenvector",
weightDeg = FALSE, normDeg = FALSE)
#?summary.microNetProps
summary(props_single_fullSet, numbNodes = 5L)
plot(props_single_fullSet,
labelScale = F,
cexLabels = 1.3,
title1 = paste("Single network with Spearman",chosenWeek, chosenTaxonomy),
showTitle = T,
cexTitle = 2.3,
nodeColor = colVector)
cexTitle = 2.3
plot(props_single_fullSet,
plot(props_single_fullSet,
labelScale = F,
cexLabels = 1.3,
title1 = paste("Single network with Spearman",chosenWeek, chosenTaxonomy),
showTitle = T,
cexTitle = 2.3)
plot(props_single_fullSet,
labelScale = F,
cexLabels = 1.3,
#title1 = paste("Single network with Spearman",chosenWeek, chosenTaxonomy),
showTitle = T,
cexTitle = 2.3)
### Script to extract data from Nathalie and Scott ###
### Metataxonomic and metabolomic data ###
# loading libraries
library(phyloseq)
library(stringr)
library(gsubfn)
# loading data
data_phys_original  <- readRDS("Data/allDataMetataxonomicNCLTEE.rds")
# The actual data
df.otu.metatax      <- as.data.frame(data_phys_original@otu_table)
# The results / variables
df.tax.metatax      <- as.data.frame(data_phys_original@tax_table)
# The sample IDs
sampleID.metatax    <- data_phys_original@sam_data$sample.name
## setting up dataframe similar to Katrines
df.fulldata.metax <- as.data.frame(t(df.otu.metatax))
rownames(df.fulldata.metax) = sampleID.metatax
# making string array with colnames
colnames_array_metatax <- str_c("DATA.",str_replace_na(df.tax.metatax$domain, replacement="NA"),"_",
str_replace_na(df.tax.metatax$phylum,  replacement="NA"),"_",
str_replace_na(df.tax.metatax$class,   replacement="NA"),"_",
str_replace_na(df.tax.metatax$order,   replacement="NA"),"_",
str_replace_na(df.tax.metatax$family,  replacement="NA"),"_",
str_replace_na(df.tax.metatax$genus,   replacement="NA"),"_",
str_replace_na(df.tax.metatax$species, replacement="NA"))
colnames(df.fulldata.metax) = colnames_array_metatax
# saving information on phaeobacter presence
array.phaebac.bin <- data_phys_original@sam_data$phaeobacter
# moving up to genus level or staying at species
whichTaxLevel <- "species"
if (whichTaxLevel=="genus") {
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(df.fulldata.metax)
newNames <- apply(str_split_fixed(string = origNames, pattern = "[_]",7)[,1:6],1, paste, collapse="_")
length(unique(newNames))
uniqNames=unique(newNames)
newDat=data.frame(dummy=1:NROW(df.fulldata.metax))
#j=uniqNames[1]
for(j in uniqNames) {
jIndx=grep(j,origNames )
if(length(jIndx)>1) {
newDat=cbind(newDat,rowSums(df.fulldata.metax[,jIndx]))
} else {
newDat=cbind(newDat,(df.fulldata.metax[,jIndx]))
}
}
newDat=newDat[,-1]
colnames(newDat)=uniqNames
# now making that new data into 16s data set
df.fulldata.metax = newDat
}
df.full.metax <- cbind(array.phaebac.bin,df.fulldata.metax)
#### Metabolomic data
# char_data <- read.csv("Data/metabolomic_day7,28,70.csv", stringsAsFactors = F)
# num_data <- data.frame(data.matrix(char_data))
# numeric_columns <- sapply(num_data,function(x){mean(as.numeric(is.na(x)))<0.5})
# final_data <- data.frame(num_data[,numeric_columns], char_data[,!numeric_columns])
# loading in data
df_metab_original <- read.csv("Data/metabolomic_day7,28,70.csv", header = TRUE,
sep = ";", stringsAsFactors = FALSE, na.strings = "NA", strip.white = TRUE)
df_metab_numOnly = read.csv("Data/metabolomic_day7,28,70_NumOnly.csv", header = FALSE,
sep = ";", stringsAsFactors = FALSE, strip.white = TRUE)
# naming columns and rows
colnames(df_metab_numOnly) = colnames(df_metab_original)[-1]
rownames(df_metab_numOnly) = df_metab_original[,1][-1]
# transposing the data frame
df_metab_tmp1 = as.data.frame(t(df_metab_numOnly))
# loading in metadata metabolomics sheet - changed data to .csv first, to make it work
metadata_metabolomics  <- read.csv("Data/Metadata-metabolomics.csv", fill = TRUE, header = TRUE, sep = ";")
# fetching two first rows / names
rownam_samples_metab  <- rownames(df_metab_tmp1)
addit_info_samples    <- df_metab_original[,1]
# running through the names, matching them with phyloseq naming
metab_new_names <- rownam_samples_metab
for (i in 1:length(rownam_samples_metab)) {
num_grep = as.numeric(unlist(regmatches(rownam_samples_metab[i], gregexpr("[[:digit:]]+", rownam_samples_metab[i]))))
if (length(num_grep) == 1 && num_grep > 400) {
# find which sample is talked about
metaDataRow = which(metadata_metabolomics$Ã¯..Sample.no...MCCe. == num_grep)
# checking for TDA
if(metadata_metabolomics$System[metaDataRow]=="TDA"){tdaBin="P"}else{tdaBin="D"}
# finding biorep
biorepSample  = metadata_metabolomics$Bio.Rep[metaDataRow]
# finding time
timeSample    = metadata_metabolomics$Time[metaDataRow] / 7
# inserting into new name format
metab_new_names[i] = paste(tdaBin,biorepSample,timeSample, sep = "-")
}
}
# inserting the correct names
rownames(df_metab_tmp1) = metab_new_names
# removing the additional column with information (1)
df_metab_tmp2 = df_metab_tmp1[,-c(1)]
# finding the common test IDs, to make a full dataset
commonIDs <- intersect(sampleID.metatax, metab_new_names)
#data_metab = cbind(metab_new_names,df_metab_noblanks_tmp3)
#data_metax = cbind(sampleID.metatax,df.full.metax)
data_metab = df_metab_tmp2
data_metax = df.full.metax
# only keeping the relevant testIDs
df_metab = data_metab[metab_new_names %in% commonIDs,]
df_metax = data_metax[sampleID.metatax %in% commonIDs,]
# normalizing the metabolomic data, percentage-based according to max peak overall
df_metab_tmp3 = as.data.frame(lapply(df_metab, function(x){x/max(df_metab)}))
colnames(df_metab_tmp3) = colnames(df_metab)
rownames(df_metab_tmp3) = rownames(df_metab)
df_metab = df_metab_tmp3[sort(commonIDs, decreasing = FALSE),]
df_metax = df_metax[sort(commonIDs, decreasing = FALSE),]
complete_data <- cbind(df_metax, df_metab, deparse.level = 1)
# jogging around, finding max metabolite peak
#maxValuesMetab <- unlist(apply(df_metab, 2, max))
#max(maxValuesMetab)
# loading libraries
library(dbscan)
library(fpc)
library(factoextra)
View(complete_data)
data = complete_data
phaobacBin <- data$array.phaebac.bin
inData = subset(data, select = -c(array.phaebac.bin))
#rownames(inData) <- testID
rownames(inData) <- commonIDs
# making DBSCAN model
inDataScale=data.frame(scale(inData, center = T, scale = T))
findEpsi(t(inDataScale), minRange = 8, maxRange = 11, steps = .1 ,maxY=15,minP = 3)
source("Programming/Functions.R")
findEpsi(t(inDataScale), minRange = 8, maxRange = 11, steps = .1 ,maxY=15,minP = 3)
View(inData)
which(inData == "NA")
View(inDataScale)
View(inDataScale)
View(t(inData))
data = complete_data
phaobacBin <- data$array.phaebac.bin
inData = subset(data, select = -c(array.phaebac.bin))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame),
start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
rm(list=setdiff(ls(), "outData"))
print("Filtering done...")
return(outData)
data_filtering <- function(data) {
# Gonna write it out first, then make it into a function
#inData = data
#testID  <- inData$testID
#OUA     <- inData$OUA
#inData = subset(inData, select = -c(testID, OUA))
data = complete_data
phaobacBin <- data$array.phaebac.bin
inData = subset(data, select = -c(array.phaebac.bin))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame),
start = list(Vm=max(ratioData), K=max(time_frame) / 2))
#mmModel <- nls(sort(ratioData) ~ Vm/(1+exp(-growthRate*(K+time_frame))),
#              start = list(Vm=max(ratioData), K=max(time_frame)/2, growthRate=0.5))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
#abline(h = threshold_ratio)
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
rm(list=setdiff(ls(), "outData"))
print("Filtering done...")
return(outData)
}
View(outData)
data = outData
inData = data
# making DBSCAN model
inDataScale=data.frame(scale(inData, center = T, scale = T))
View(inDataScale)
findEpsi(t(inDataScale), minRange = 8, maxRange = 11, steps = .1 ,maxY=15,minP = 3)
source("Programming/Functions.R")
findEpsi(t(inDataScale), minRange = 8, maxRange = 11, steps = .1 ,maxY=15,minP = 3)
findEpsi(t(inDataScale), minRange = 0, maxRange = 100, steps = 1 ,maxY=15,minP = 3)
findEpsi(t(inDataScale), minRange = 0, maxRange = 10, steps = 1 ,maxY=15,minP = 3)
findEpsi(t(inDataScale), minRange = 2, maxRange = 5, steps = .1 ,maxY=15,minP = 3)
DB=dbscan::dbscan(t(inDataScale),4.3,3); print(DB)
clusVars=data.frame(Clus=DB$cluster,Vars=colnames(inDataScale))
#sub <- subset(clusVars, Clus==5); print(sub)
subset(clusVars, Clus==5)
#sub <- subset(clusVars, Clus==5); print(sub)
subset(clusVars, Clus==1)
DB
#sub <- subset(clusVars, Clus==5); print(sub)
subset(clusVars, Clus==2)
#sub <- subset(clusVars, Clus==5); print(sub)
subset(clusVars, Clus==3)
#sub <- subset(clusVars, Clus==5); print(sub)
subset(clusVars, Clus==4)
#sub <- subset(clusVars, Clus==5); print(sub)
subset(clusVars, Clus==1)
# making PCA model
pcaModel <- prcomp(data, center = TRUE, scale = TRUE)
# making scree plot, to visualize eigenvalues
fviz_eig(pcaModel)
# making a graph of the variables
# visualizing the correlation between the variables
# positive correlated variables point to the same side of the plot
# negative correlated variables point to opposite sides of the plot
fviz_pca_var(pcaModel,
col.var = "contrib", # Color by contributions to the PC
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
repel = FALSE     # Avoid text overlapping
#,select.var = list(name = rownames(inData))
)
plot(pcaModel$x[,1], pcaModel$x[,2])
plot(pcaModel$x[,1], pcaModel$x[,3])
####################################
pca <- prcomp((data), scale=TRUE)
## plot pc1 and pc2
plot(pca$x[,1], pca$x[,2])
plot(pca$x[,1], pca$x[,3])
## make a scree plot
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
abline(h=1/ncol(inData)*100, col='red')
# other way! - making scree plot, to visualize eigenvalues
fviz_eig(pcaModel)
## now make a fancy looking plot that shows the PCs and the variation:
library(ggplot2)
pca.data <- data.frame(Sample=rownames(pca$x),
X=pca$x[,1],
Y=pca$x[,2])
pca.data
ggplot(data=pca.data, aes(x=X, y=Y, label=Sample)) +
geom_text() +
xlab(paste("PC1 - ", pca.var.per[1], "%", sep="")) +
ylab(paste("PC2 - ", pca.var.per[2], "%", sep="")) +
theme_bw() +
ggtitle("My PCA Graph")
## get the name of the top 10 measurements (genes) that contribute
## most to pc1.
loading_scores <- pca$rotation[,1]
gene_scores <- abs(loading_scores) ## get the magnitudes
gene_score_ranked <- sort(gene_scores, decreasing=TRUE)
top_10_genes <- names(gene_score_ranked[1:10])
top_10_genes ## show the names of the top 10 genes
pca$rotation[top_10_genes,1] ## show the scores (and +/- sign)
