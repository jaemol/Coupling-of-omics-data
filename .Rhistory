for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and fecundicity
A[1,17] <- 208.1
eigen(A) # notice the real eigenvector is somewhere in the middle
# dominant real eigenvalue is still >1, which mena that the population will increase over time.
# however not with the same speed as before.
# eigenvector shows that the population will be dominated by younger individuals
n <- numeric(17)
n[1] <- 1000
sn <- numeric(200)
sn[1] <- sum(n)
for (i in 2:200)
{
n <- A %*% n
sn[i] <- sum(n)
}
plot(log10(sn))
##### 12 ####
lambda <- numeric(121)
for (conc in 0:120)
{
A <- matrix(0,nrow=17,ncol=17)
# fill the sub-diagonal
# eggs and L1
s <- 0.836
a <- 8.478
b <- 0.282
for (i in 1:4) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc))
for (i in 5:6) A[i+1,i] <- qfun(conc,2)
for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and the reamining survival and fecundicity
A[1,17] <- 208.1
ev <- eigen(A)
lambda[conc+1] <- max(abs(ev$values))
}
plot(0:120,lambda,xlab = "methiocarb conc in mu g / L", ylab="lambda")
grid()
# 23 Âµg/l is sufficient to reduce lambda below 1.
##### 13 ####
A <- matrix(0,nrow=17,ncol=17)
s <- 0.836
a <- 8.478
b <- 0.282
conc <- 0
for (i in 1:4) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc))
for (i in 5:6) A[i+1,i] <- qfun(conc,2)
for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and the reamining survival and fecundicity
A[1,17] <- 208.1
# find dominant right and left eigenvectors + lambda
ev <- eigen(A)
lev <- eigen(t(A))
# notice that k denotes the eigenvalue with max real part.
k <- which(ev$values==max(abs(Re(ev$values))))
w <- Re(ev$vector[,k])
lambda <- Re(ev$values[k])
k <- which(lev$values==max(abs(Re(lev$values))))
v <- Re(lev$vector[,k])
ndim <- dim(A)[1]
# calculate elasticity matrix
e <- matrix(NA,ncol=ndim,nrow=ndim)
for (i in 1:ndim)
{
for (j in 1:ndim)
{
e[i,j] <- v[i]*w[j] / v%*%w * A[i,j] / lambda
}
}
e
library(plot3D)
hist3D(z=e, x=1:ndim, y=1:ndim, ticktype = "detailed",xlab="future",ylab="present",
border="black",col="white",space=0.15,zlab="elasticity")
# elasticity matrix shows that all steps in the lifecycle are equally important.
##### 14 ####
A <- matrix(0,nrow=17,ncol=17)
s <- 0.836
a <- 8.478
b <- 0.282
conc <- 60 # or 30
for (i in 1:4) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc))
for (i in 5:6) A[i+1,i] <- qfun(conc,2)
for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and the reamining survival and fecundicity
A[1,17] <- 208.1
# find dominant right and left eigenvectors + lambda
ev <- eigen(A)
lev <- eigen(t(A))
k <- which(ev$values==max(abs(Re(ev$values))))
w <- Re(ev$vector[,k])
lambda <- Re(ev$values[k])
k <- which(lev$values==max(abs(Re(lev$values))))
v <- Re(lev$vector[,k])
ndim <- dim(A)[1]
# calculate elasticity matrix
e <- matrix(NA,ncol=ndim,nrow=ndim)
for (i in 1:ndim)
{
for (j in 1:ndim)
{
e[i,j] <- v[i]*w[j] / v%*%w * A[i,j] / lambda
}
}
e
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
# loading in the sources of other functions
source("G:/Mit drev/Specialeprojekt/Programming/extracting_data_KAT.R")
source("G:/Mit drev/Specialeprojekt/Programming/data_filtering.R")
source("G:/Mit drev/Specialeprojekt/Programming/Functions.R")
# loading in the data
list[data, len_16s, len_qpcr] <- extracting_data_KAT()
?grep
infile = data
?var
# loading libraries
library(factoextra) # used to create ggplot2-based visualization
setwd("~/GitHub/Coupling-of-omics-data")
data_16s  <- read.table("Testdata/allData_16S_cleaned.txt")
data_qpcr <- read.table("Testdata/allData_qPCR_cleaned.txt")
# next, include only testID (PIG_DATE), OUA and the results of the analyses
pure_data_16s   <- subset(data_16s, select = -c(PIG, DATE, WEANING_TIME, GROUP, Newlytreated,
WEEK, SAMPLEWEEK, Florkem, Metacam, Zactran,
Antibiotic, Treatment_group, Treatment_date, CorrectedGroup,
PIG.1, DATE.1, OUA.1, PIG_DATE.1, OriginalNAME))
pure_data_qpcr  <- subset(data_qpcr, select = -c(PIG, DATE, WEANING_TIME, GROUP, Newlytreated,
WEEK, SAMPLEWEEK, Florkem, Metacam, Zactran,
Antibiotic, Treatment_group, Treatment_date, CorrectedGroup, sample))
# extract two arrays of testIDs, and find which are common between them both
testID_16s  <- pure_data_16s$PIG_DATE
testID_qpcr <- pure_data_qpcr$PIG_DATE
common_IDs  <- intersect(testID_16s, testID_qpcr)
# only keep relevant testIDs
pure_data_16s   = pure_data_16s[data_16s$PIG_DATE %in% common_IDs, ]
pure_data_qpcr  = pure_data_qpcr[data_qpcr$PIG_DATE %in% common_IDs, ]
# sort the datasets based testID, append whilst removing 1 edition of testID and OUA
pure_data_16s   = pure_data_16s[order(pure_data_16s$PIG_DATE), ]
pure_data_qpcr  = pure_data_qpcr[order(pure_data_qpcr$PIG_DATE), ]
pure_data_qpcr  = subset(pure_data_qpcr, select = -c(PIG_DATE, OUA))
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(data_16s)
newNames <- apply(stringr::str_split_fixed(string = origNames, pattern = "_",8)[,1:6],1, paste, collapse="_")
head(data_16s, 2)
head(data_16s[1:3], 2)
head(pure_data_16s[1:3], 2)
# extracting testID and OUA from 16s data
testID  <- pure_data_16s$PIG_DATE
OUA     <- pure_data_16s$OUA
pure_data_16s = subset(pure_data_16s, select = -c(PIG_DATE, OUA))
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(data_16s)
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(pure_data_16s)
newNames <- apply(stringr::str_split_fixed(string = origNames, pattern = "_",8)[,1:6],1, paste, collapse="_")
length(unique(newNames))
uniqNames=unique(newNames)
newDat=data.frame(dummy=1:NROW(pure_data_16s))
#j=uniqNames[1]
for(j in uniqNames) {
jIndx=grep(j,origNames )
if(length(jIndx)>1) {
newDat=cbind(newDat,rowSums(pure_data_16s[,jIndx]))
} else {
newDat=cbind(newDat,(pure_data_16s[,jIndx]))
}
}
newDat=newDat[,-1]
colnames(newDat)=uniqNames
head(newDat, 10)
View(newDat)
# now making that new data into 16s data set
pure_data_16s = newDat
# now, append the datasets, to get one set, along with testID and OUA
complete_data <- cbind(testID, OUA, pure_data_16s, pure_data_qpcr, deparse.level = 1)
View(complete_data)
# keeping the length of 16s and qpcr data - JUST the results, no ID or OUA
len_16s   <- length(data_16s[1,]) - 2
len_qpcr  <- length(data_qpcr[1,])
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
## Stationary
source("Programming/extracting_data_KAT.R")
source("Programming/data_filtering.R")
# loading in the data
list[data, len_16s, len_qpcr] <- extracting_data_KAT()
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(PIG_DATE, OUA))
inData = subset(inData, select = -c(testID, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
#mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
mmModel <- nls(sort(ratioData) ~ 1/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
#mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
mmModel <- nls(sort(ratioData) ~ Vm/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
#abline(h = threshold_ratio)
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
rm(list=setdiff(ls(), "outData"))
print("Filtering done...")
data <- outData
# loading libraries
library(fpc)
library(dbscan)
library(factoextra)
# setting seed
set.seed(123)
# excluding testID and OUA (antibiotics used or not)
inData = data
testID  <- inData$testID
OUA     <- inData$OUA
inData = subset(inData, select = -c(testID, OUA))
findEpsi=function(L, minRange=0,maxRange=3, steps=0.1, maxY=200, minP=10) {
plot(0,0, col=0,xlim=c(minRange,maxRange), ylim=c(0,maxY))
legend("topright", legend = c("nClust","nOutliers"), col=1:2, pch=16)
for(i in seq(minRange,maxRange,steps)) {
DB=dbscan::dbscan(L,i,minP)
points(i,length(unique(DB$cluster)), pch=16, col=1)
points(i,length(which(DB$cluster==0)), pch=16,col=2)
}
}
inDataScale=scale(inData, center = T, scale = T)
findEpsi(inDataScale, minRange = 0, maxRange = 100, steps = 10,maxY=30)
dbscan::dbscan(scale(inData),10000,10)
findEpsi(inDataScale, minRange = 0, maxRange = 3, steps = 0.1,maxY=200)
findEpsi(inDataScale, minRange = 0, maxRange = 120, steps = 0.1,maxY=200)
dbscan::dbscan(scale(inData),10000,10)
findEpsi(inDataScale, minRange = 0, maxRange = 25, steps = 0.1 ,maxY=200)
?dbscan
findEpsi
dbscan::dbscan(scale(inData),10,10)
dbscan::dbscan(scale(inData),2,10)
dbscan::dbscan(scale(inData),20,10)
dbscan::dbscan(scale(inData),25,10)
dbscan::dbscan(scale(inData),50,10)
dbscan::dbscan(inDataScale,50,10)
dbscan::dbscan(inDataScale,11,10)
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 7, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 2, nstart = 25)
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 4, nstart = 25)
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 2, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 1, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 3, nstart = 25)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 3, nstart = 50)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
# selecting arbitrary cluster number, just for visualization
kmeans_model <- kmeans(inData, 3, nstart = 10)
# generating cluster
fviz_cluster(kmeans_model, inData, frame = FALSE, geom = "point")
