b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and fecundicity
A[1,17] <- 208.1
eigen(A) # notice the real eigenvector is somewhere in the middle
# dominant real eigenvalue is still >1, which mena that the population will increase over time.
# however not with the same speed as before.
# eigenvector shows that the population will be dominated by younger individuals
n <- numeric(17)
n[1] <- 1000
sn <- numeric(200)
sn[1] <- sum(n)
for (i in 2:200)
{
n <- A %*% n
sn[i] <- sum(n)
}
plot(log10(sn))
##### 12 ####
lambda <- numeric(121)
for (conc in 0:120)
{
A <- matrix(0,nrow=17,ncol=17)
# fill the sub-diagonal
# eggs and L1
s <- 0.836
a <- 8.478
b <- 0.282
for (i in 1:4) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc))
for (i in 5:6) A[i+1,i] <- qfun(conc,2)
for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and the reamining survival and fecundicity
A[1,17] <- 208.1
ev <- eigen(A)
lambda[conc+1] <- max(abs(ev$values))
}
plot(0:120,lambda,xlab = "methiocarb conc in mu g / L", ylab="lambda")
grid()
# 23 Âµg/l is sufficient to reduce lambda below 1.
##### 13 ####
A <- matrix(0,nrow=17,ncol=17)
s <- 0.836
a <- 8.478
b <- 0.282
conc <- 0
for (i in 1:4) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc))
for (i in 5:6) A[i+1,i] <- qfun(conc,2)
for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and the reamining survival and fecundicity
A[1,17] <- 208.1
# find dominant right and left eigenvectors + lambda
ev <- eigen(A)
lev <- eigen(t(A))
# notice that k denotes the eigenvalue with max real part.
k <- which(ev$values==max(abs(Re(ev$values))))
w <- Re(ev$vector[,k])
lambda <- Re(ev$values[k])
k <- which(lev$values==max(abs(Re(lev$values))))
v <- Re(lev$vector[,k])
ndim <- dim(A)[1]
# calculate elasticity matrix
e <- matrix(NA,ncol=ndim,nrow=ndim)
for (i in 1:ndim)
{
for (j in 1:ndim)
{
e[i,j] <- v[i]*w[j] / v%*%w * A[i,j] / lambda
}
}
e
library(plot3D)
hist3D(z=e, x=1:ndim, y=1:ndim, ticktype = "detailed",xlab="future",ylab="present",
border="black",col="white",space=0.15,zlab="elasticity")
# elasticity matrix shows that all steps in the lifecycle are equally important.
##### 14 ####
A <- matrix(0,nrow=17,ncol=17)
s <- 0.836
a <- 8.478
b <- 0.282
conc <- 60 # or 30
for (i in 1:4) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc))
for (i in 5:6) A[i+1,i] <- qfun(conc,2)
for (i in 7:9) A[i+1,i] <- qfun(conc,3)
for (i in 10:15) A[i+1,i] <- qfun(conc,4)
#puppae
s <- 1
a <- 8.749
b <- 0.499
for (i in 16:16) A[i+1,i] <- s*(1 + exp(a))/(exp(a)+exp(b*conc)) * qfun(conc,4)
# and the reamining survival and fecundicity
A[1,17] <- 208.1
# find dominant right and left eigenvectors + lambda
ev <- eigen(A)
lev <- eigen(t(A))
k <- which(ev$values==max(abs(Re(ev$values))))
w <- Re(ev$vector[,k])
lambda <- Re(ev$values[k])
k <- which(lev$values==max(abs(Re(lev$values))))
v <- Re(lev$vector[,k])
ndim <- dim(A)[1]
# calculate elasticity matrix
e <- matrix(NA,ncol=ndim,nrow=ndim)
for (i in 1:ndim)
{
for (j in 1:ndim)
{
e[i,j] <- v[i]*w[j] / v%*%w * A[i,j] / lambda
}
}
e
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
# loading in the sources of other functions
source("G:/Mit drev/Specialeprojekt/Programming/extracting_data_KAT.R")
source("G:/Mit drev/Specialeprojekt/Programming/data_filtering.R")
source("G:/Mit drev/Specialeprojekt/Programming/Functions.R")
# loading in the data
list[data, len_16s, len_qpcr] <- extracting_data_KAT()
?grep
infile = data
?var
# loading libraries
library(factoextra) # used to create ggplot2-based visualization
setwd("~/GitHub/Coupling-of-omics-data")
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(PIG_DATE, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
install.packages("proto")
install.packages("proto")
### This is the main programming script for handling the data ###
rm(list = ls())
# loading libraries
#library(dplyr)
library(gsubfn)
## Stationary
source("Programming/extracting_data_KAT.R")
source("Programming/data_filtering.R")
# loading in the data
list[data, len_16s, len_qpcr] <- extracting_data_KAT()
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(PIG_DATE, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
which(ratioData == max(ratioData)*0.9)
max(ratioData)*0.9
which(ratioData == max(ratioData)*0.95)
which(ratioData => max(ratioData)*0.95)
which(ratioData >= max(ratioData)*0.95)
which(ratioData >= max(ratioData)*0.95)[1]
which(ratioData >= max(ratioData)*0.95)[1:10]
ratioData[48]
time_frame[48]
which(sort(ratioData) >= max(ratioData)*0.95)[1:10]
ratioData[1335]
max(ratioData)*0.95
a_linearFit <- (which(sort(ratioData)>=max(ratioData)*0.95)[1] - min(ratioData)) / (max(time_frame)-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
min(ratioData)
which(sort(ratioData)>=max(ratioData)*0.95)[1]
ratioData[1335]
sort(ratioData[1335])
sort(ratioData[1336])
sort(ratioData[1337])
temp_x = which(sort(ratioData)>=max(ratioData)*0.95)[1]
temp_y = Vmax*temp_x / (K+temp_x)
a_linearFit <- (temp_y - min(ratioData)) / (max(time_frame)-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
temp_y
min(ratioData)
temp_y
max(time_frame)
a_linearFit <- (temp_y - min(ratioData)) / (temp_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
i=kneePoints_collected[400]
for (i in kneePoints_collected) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
for (i in kneePoints_collected) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- ratioData[longest_x]
threshold_ratio
which(ratioData <= threshold_ratio)
length(which(ratioData <= threshold_ratio))
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
temp_x = which(sort(ratioData)>=max(ratioData)*0.90)[1]
temp_y = Vmax*temp_x / (K+temp_x)
a_linearFit <- (temp_y - min(ratioData)) / (temp_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
temp_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
temp_y = Vmax*temp_x / (K+temp_x)
a_linearFit <- (temp_y - min(ratioData)) / (temp_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
i=kneePoints_collected[400]
for (i in kneePoints_collected) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- ratioData[longest_x]
threshold_ratio
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
threshold_ratio
longest_x
longest_x         <- 0
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
for (i in kneePoints_collected) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
longest_x
kneePoints_collected
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in kneePoints_collected<max_x) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in kneePoints_collected<max_x) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
threshold_ratio
longest_x
longest_distance
which(kneePoints_collected<max_x)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
threshold_ratio
longest_x
point(longest_x, threshold_ratio)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
rm(list=setdiff(ls(), "data"))
# Gonna write it out first, then make it into a function
inData = data
testID  <- inData$PIG_DATE
OUA     <- inData$OUA
inData = subset(inData, select = -c(PIG_DATE, OUA))
# if the variance is equal to zero, then its a zero-column
#which(unname(apply(inData, 2, var))==0)
throwAway <- which(apply(inData, 2, var)==0)[]
# dropping columns by index
inData = inData[, -throwAway]
rm(throwAway) # to save memory
print("Generating metadata...")
## finding different interesting information on the set
## mean, var, STD, ratio of zeros,
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
# fitting Michaelis-Menten function to data
print("Fitting Michaelis-Menten function...")
time_frame <- seq(from = 1, to = length(ratioData), by = 1)
mmModel <- nls(sort(ratioData) ~ Vm*time_frame/(K+time_frame), start = list(Vm=max(ratioData), K=max(time_frame) / 2))
# parameters estimated including confidence interval
coef(mmModel)
confint(mmModel, level = 0.9)
# defining to estimated parameters
Vmax = coef(mmModel)[[1]]
K    = coef(mmModel)[[2]]
# visualizing the model in relation to ratio Data
plot(sort(ratioData) ~ time_frame, col = "grey")
lines(predict(mmModel) ~ time_frame, lwd = 3, col = "dark red")
# finding local maxima, potentiel knee points
kneePoints_collected <- which(diff(sign(diff(ratioData)))==-2)+1
# finding the linear fit from origo to asymptote
a_linearFit <- max(ratioData) / max(time_frame)
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
max_x = which(sort(ratioData)>=max(ratioData)*0.97)[1]
max_y = Vmax*max_x / (K+max_x)
a_linearFit <- (max_y - min(ratioData)) / (max_x-min(time_frame))
b_linearFit <- min(ratioData) - a_linearFit*min(time_frame)
abline(a = b_linearFit, b=a_linearFit)
# using distance formular on all knee points to line
# distance formula: d = abs(a*x+b-y)/sqrt(a^2+1)
longest_distance  <- 0
longest_x         <- 0
#i=kneePoints_collected[400]
for (i in which(kneePoints_collected<=max_x)) {
temp_x = i
temp_y = Vmax*temp_x / (K+temp_x)
#points(temp_x, temp_y)
dist = abs(a_linearFit*temp_x + b_linearFit-temp_y)/sqrt(a_linearFit^2+1)
if (dist >= longest_distance) {longest_distance = dist; longest_x = temp_x}
}
# defining the cutoff threshold
threshold_ratio <- Vmax*longest_x / (K+longest_x)
points(longest_x, threshold_ratio)
abline(h = threshold_ratio)
# if the ratioData is above the found cutoff, it is to be removed
keepIn <- which(ratioData <= threshold_ratio)
# dropping columns by index
sprintf("Filtering out data with zero-ratio above %f...", threshold_ratio)
inData = inData[, keepIn]
rm(keepIn) # to save memory
meanData  <- unname(apply(inData, 2, mean))
stdData   <- unname(apply(inData, 2, sd))
varData   <- unname(apply(inData, 2, var))
ratioData <- unname(apply(inData, 2, function(x){length(which(x==0))/length(x)}))
CVData    <- unname(apply(inData, 2, function(x){sd(x)/mean(x)}))
plot(sort(meanData), main = "Mean Data")
plot(sort(stdData), main = "Standard Variations")
plot(sort(varData), main = "Variance")
plot(sort(ratioData), main = "Singleton ratios")
plot(sort(CVData), main = "Relative standard deviation")
inData = cbind(testID, OUA, inData)
outData = inData
rm(list=setdiff(ls(), "outData"))
print("Filtering done...")
head(outData[3])
data_16s  <- read.table("Testdata/allData_16S_cleaned.txt")
data_qpcr <- read.table("Testdata/allData_qPCR_cleaned.txt")
# next, include only testID (PIG_DATE), OUA and the results of the analyses
pure_data_16s   <- subset(data_16s, select = -c(PIG, DATE, WEANING_TIME, GROUP, Newlytreated,
WEEK, SAMPLEWEEK, Florkem, Metacam, Zactran,
Antibiotic, Treatment_group, Treatment_date, CorrectedGroup,
PIG.1, DATE.1, OUA.1, PIG_DATE.1, OriginalNAME))
pure_data_qpcr  <- subset(data_qpcr, select = -c(PIG, DATE, WEANING_TIME, GROUP, Newlytreated,
WEEK, SAMPLEWEEK, Florkem, Metacam, Zactran,
Antibiotic, Treatment_group, Treatment_date, CorrectedGroup, sample))
# extract two arrays of testIDs, and find which are common between them both
testID_16s  <- pure_data_16s$PIG_DATE
testID_qpcr <- pure_data_qpcr$PIG_DATE
common_IDs  <- intersect(testID_16s, testID_qpcr)
# only keep relevant testIDs
pure_data_16s   = pure_data_16s[data_16s$PIG_DATE %in% common_IDs, ]
pure_data_qpcr  = pure_data_qpcr[data_qpcr$PIG_DATE %in% common_IDs, ]
# sort the datasets based testID, append whilst removing 1 edition of testID and OUA
pure_data_16s   = pure_data_16s[order(pure_data_16s$PIG_DATE), ]
pure_data_qpcr  = pure_data_qpcr[order(pure_data_qpcr$PIG_DATE), ]
pure_data_qpcr  = subset(pure_data_qpcr, select = -c(PIG_DATE, OUA))
# moving up in taxonomy for the 16s data, going from species to genus
origNames <- colnames(data_16s)
origNames
newNames <- apply(stringr::str_split_fixed(string = origNames, pattern = "_",8)[,1:6],1, paste, collapse="_")
newNames
length(unique(newNames))
uniqNames=unique(newNames)
newDat=data.frame(dummy=1:NROW(inData))
newDat=data.frame(dummy=1:NROW(uniqNames))
newDat=data.frame(dummy=1:NROW(pure_data_16s))
#j=uniqNames[1]
for(j in uniqNames) {
jIndx=grep(j,origNames )
if(length(jIndx)>1) {
newDat=cbind(newDat,rowSums(inData[,jIndx]))
} else {
newDat=cbind(newDat,(inData[,jIndx]))
}
}
#j=uniqNames[1]
for(j in uniqNames) {
jIndx=grep(j,origNames )
if(length(jIndx)>1) {
newDat=cbind(newDat,rowSums(pure_data_16s[,jIndx]))
} else {
newDat=cbind(newDat,(pure_data_16s[,jIndx]))
}
}
